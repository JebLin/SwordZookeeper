
首先介绍一下，Zookeeper中有一种节点叫做顺序节点，故名思议，假如我们在/lock/目录下创建节3个点，
ZooKeeper集群会按照提起创建的顺序来创建节点，节点分别为/lock/0000000001、/lock/0000000002、/lock/0000000003。

ZooKeeper中还有一种名为临时节点的节点，临时节点由某个客户端创建，当客户端与ZooKeeper集群断开连接。则该节点自动被删除。

算法思路：
    对于加锁操作，可以让所有客户端都去/lock目录下创建临时、顺序节点，
    如果创建的客户端发现自身创建节点序列号是/lock/目录下最小的节点，则获得锁。
    否则，监视比自己创建节点的序列号小的节点（当前序列在自己前面一个的节点），进入等待。解锁操作，只需要将自身创建的节点删除即可。

考虑有成百上千客户端的情况，所有的客户端都在尝试获得锁，每个客户端都会在锁znode上设置一个观察，用于捕捉子节点的变化。
每次锁被释放或另外一个进程开始申请获取锁的时候，观察都会被触发并且每个客户端都会收到一个通知。

“羊群效应“：
    就是指大量客户端收到同一事件的通知，但实际上只有很少一部分客户端需要处理这一事件。
    在这种情况下，只有一个客户端会成功地获取锁，但是维护过程及向所有客户端发送观察事件会产生峰值流量，这会对ZooKeeper服务器造成压力。
方案解决方案
    为了避免出现羊群效应，我们需要优化通知的条件。
    关键在于只有在前一个顺序号的子节点消失时才需要通知下一个客户端，而不是删除（或创建）任何子节点时都需要通知。
    在我们的例子中，如果客户端创建了znode /leader/lock-1、/leader/lock-2和／leader/lock-3，
    那么只有当/leader/lock-2消失时才需要通知／leader/lock-3对照的客户端；/leader/lock-1消失或有新的znode /leader/lock-4加入时，
    不需要通知该／leader/lock-3对照的客户端。




Lock:

DistributedLock
ZkClientExt
BaseDistributedLock
SimpleDistributedLockMutex
DistributedLockMutex
TestDistributedLock



